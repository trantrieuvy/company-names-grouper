{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OoPRuOrdlPlc"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file = json.load(open(\"company_cleaning_minroot_1000.json\", \"r\"))\n",
    "validation_file = json.load(open(\"company_cleaning_minroot_val_disjoint1000.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BEWbHelvmP_3",
    "outputId": "b7002796-a12c-427b-df3d-d3b84c22ddcf"
   },
   "outputs": [],
   "source": [
    "!pip install unsloth trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zOTIpU5hm8P4",
    "outputId": "af77d27c-8f2e-4b35-c33d-2dfb16d05f07"
   },
   "outputs": [],
   "source": [
    "# For GPU check\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540,
     "referenced_widgets": [
      "b13b3a1e4c224e90a289de72c7710ef1",
      "55e15e74bb084ea680c6c32b73eeb0ed",
      "b68e00991aba48a98dadee301426e783",
      "210d305dcee54f08a2853c98cb18ea86",
      "abf2201ca10d49e794dbe04b1d512ca6",
      "293988acfe9e496d9ee8bd6f57fb5516",
      "5403adab53a042b09d1eb6135d790b42",
      "f5cb8560b26a47e68077576683b1d7ab",
      "278fbb57959341c49a59da40b8c9d11f",
      "cc7b607b852c48dd83e813bfe3a0dd82",
      "3788c1bd06624ba280f58041a32ecb0b",
      "40b848af1cf147c3b22989385e79a9a8",
      "18f5c641aa9447c599076a3a828a2da9",
      "c74c467fa61f4e799cda9b7f31b1b775",
      "2c64e73de30345d7a703e2c119c648b5",
      "c8155dbdf737442d80e7b8e0e293ce30",
      "d08952a06bdd4cfaa58b18d2aa889d7e",
      "f071d2f0b4264518b58a241888b3a8f3",
      "9f9771e970dc401d8097e5dec94da786",
      "cfcb778d8d96400c9d2349b2c453bd2c",
      "c05b7d1488754c0caf00f7e90aa9bd03",
      "e847c263916444b5bdd65654eea9f98d",
      "b3821bd6c0954911b581096c27c4c265",
      "3b8ca10707d740199e1c1bfc32dd825c",
      "0aed6c39f3c84a75a95c7f1fe740266b",
      "f9c40fcc1e84478aacfb812848f51c2c",
      "8d9e546e70ca4f1e8e88ac8e5dd36535",
      "2e9c08f667ef4131af82ff577aada898",
      "d2d0c443609049d394a3eafaced4ec57",
      "5775084400274256a921fda034c0a873",
      "c2109b5fe62346c3b6feac3b1098424d",
      "0d4e5f046b374a6e81df0d110205b6dd",
      "11cc4777efd640449f2c73d9d0f52541",
      "c5b3038ac95146fcabacbc9759f429f7",
      "c08ee609ca704dd0b35ac727697ced5d",
      "159f04bf2ddd4ca0b51a5f3d6d585840",
      "69977166fc7c446c873cfbd5f61a4c54",
      "708bd1b4106d4db2a5ea1109b0c717d3",
      "315d36fc612343bbaa843c1b2019bb38",
      "12829a8622fd43dc98d942cfc115d78e",
      "568b498c691d4570a6ee53e220822a39",
      "056654508b1a4b4eb816e168d97677c3",
      "61bbb2529c7745b1a746adda30622b41",
      "5da01743bbdb49bb92105c5ca5865cb7",
      "9153d74b70374714b233526ddcb4c5e4",
      "bb78e4e339154274bde15ca2752ab72f",
      "2a362c9208db427bb39b42e1ddfc0444",
      "7b88a6b4418f41cf962cbeb078e93642",
      "977760225d474f1e854dfb8deaf71919",
      "c728af1a58064198898ac10ede2364ce",
      "16cd8fce004541538a7d2615880a75f3",
      "4adecc21d4c8437d94bf45b2e8021721",
      "e300ea5de2d14292b59a56398e483b5e",
      "7df2afedae61456aac30ad05ea43cec6",
      "61ae8cdcccf74819a78cdceb91f52247",
      "0ceaca0e5c87425e9772cf489abb507d",
      "2d8266a54c0b4fe2b563d1036e41f1ff",
      "e381f0c7852a4797be9aa2042350ff11",
      "a61b443a1bc1414db5800bf34a37aea5",
      "cd179dec1b7d41af8b5e1eac53752d42",
      "dd93884033d84de5a8a53ae6d136c2d8",
      "13e393b0198743dfa59c0bb99ca6dff1",
      "687dcfc7b99c41f19a7893960c608c32",
      "325ff90568fd4da99f9a7f75b9defe8d",
      "c0a7f468c3154c03a9ca24c347b9303d",
      "c7edcde2b093484ab992a008b7b7031e",
      "353a270892cb4a24bb3f60e7fe0c18f9",
      "b8f471c5d5d345e6ba94783ddb367e8a",
      "b23463369aae4dd8944d9e94f8e20fbb",
      "8746acc8a29349639b0db1f715ee8910",
      "7c5fd8cf7a0c415fab2779b34098acc4",
      "17f636004c58459b8be9b7aeb0ae1313",
      "5cde0c46a2164161a8e600e991ad9687",
      "140c7e7aa982440390a921537726b3a1",
      "2e6009faf4ca4f92ad0f92661724abdd",
      "c6635f073c784786b68986fb473cc32c",
      "09275cc7e2704885b333a648b2d4b360"
     ]
    },
    "id": "mPhk1fgAm_u8",
    "outputId": "262ddde3-4c76-4fde-d6db-e968a0c24e56"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model_name = \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\"\n",
    "\n",
    "max_seq_length = 64\n",
    "dtype = None  # let Unsloth automatically detect the best precision\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zn0fPK9kn5QT"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def format_prompt(example):\n",
    "    return f\"### Input: {example['input']}\\n### Output: {json.dumps(example['label'])}<|endoftext|>\"\n",
    "\n",
    "formatted_data = [format_prompt(item) for item in file]\n",
    "dataset = Dataset.from_dict({\"text\": formatted_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtD3WkQgiD9O",
    "outputId": "6fda83a8-0058-4328-f04f-eeae7742ce2b"
   },
   "outputs": [],
   "source": [
    "formatted_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s5ck1F_VqBOM",
    "outputId": "f3e7a1b9-f94b-4b47-834d-29e1b8e1c75b"
   },
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,  # LoRA rank suggested as suffice in the LoRA paper\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"v_proj\", # adapters on these projections perform best sugested by the LoRA paper\n",
    "        \"o_proj\", \"k_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", # in newer research, it is recommended to apply LoRA to all layers\n",
    "    ],\n",
    "    lora_alpha=16,  # LoRA scaling factor (usually 2x rank), controls the strength of the fine-tuned adjustments\n",
    "    lora_dropout=0,  # regularization that helps prevent overfitting by randomly setting a fraction of LoRA activations to zero during each training step. No dropout because of our small clean dataset\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized version, reduces memory usage by an extra 30% and supports extremly long context fine-tunes\n",
    "    random_state=12,\n",
    "    use_rslora=False, # apply the effective scaling as the standard lora_alpha / r\n",
    "    loftq_config=None, # advanced technique proposed in LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77,
     "referenced_widgets": [
      "5a56727b3e1445bdac2b3b86b71a0ea1",
      "073d2361857e401fa416819dd4b40db2",
      "8bbe857c6f5a40afa448e16585d484bd",
      "02958661079546e2ba5905e96ba9ce79",
      "9a967126d6a24114b8b31a96b1ef88ee",
      "2055f7f1072548c18b4853f6226cb37a",
      "fe08dc5226b04d7aa650dd8eea8d0489",
      "87c3b0278f8f4752be9d239c47ac51b9",
      "04a92a3ecacd48fa93383c650f83df72",
      "5f87dba9297445fa8c42a69163be026e",
      "fcd083e3d8844eb299b78625196f8d00"
     ]
    },
    "id": "0QeQBIeNqIAx",
    "outputId": "fcefb52e-a815-4b90-ae9b-a6f61a0e91e0"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training arguments optimized for Unsloth\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\", # field in dataset containing the full text samples\n",
    "    max_seq_length=max_seq_length,  # maximum number of tokens per input sample\n",
    "    dataset_num_proc=2, # use 2 CPU processes for parallel tokenization\n",
    "    args=TrainingArguments(\n",
    "        # Optimization settings\n",
    "        learning_rate=2e-4,  # high LR works well for LoRA since only small adapter weights are trained\n",
    "        optim=\"adamw_8bit\", # memory-efficient 8-bit AdamW optimizer\n",
    "        weight_decay=0.01, # small L2 regularization to prevent overfitting\n",
    "        lr_scheduler_type=\"linear\", # linearly decreases LR from initial value to 0 over training\n",
    "\n",
    "        # Warmup\n",
    "        warmup_steps=10, # gradually increase LR from 0 to target LR over first 10 steps (helps stabilize training)\n",
    "\n",
    "        # Batch\n",
    "        per_device_train_batch_size=2, # number of samples processed per device (GPU) per step\n",
    "        gradient_accumulation_steps=4,  # accumulate gradients over 4 steps before updating weights\n",
    "                                        # ->Effective batch size = per_device_train_batch_size * gradient_accumulation_steps = 2 * 4 = 8\n",
    "\n",
    "        # Epochs and precision\n",
    "        num_train_epochs=3, # how many times to iterate over the entire dataset\n",
    "        fp16=not torch.cuda.is_bf16_supported(),  # use 16-bit floating point precision if bf16 not available\n",
    "        bf16=torch.cuda.is_bf16_supported(), # use bfloat16 if GPU supports it (e.g., A100, L4, T4)\n",
    "\n",
    "        # Logging\n",
    "        logging_steps=25, # log loss and metrics every 25 steps\n",
    "        seed=12,\n",
    "\n",
    "        # Saving\n",
    "        output_dir=\"outputs\",\n",
    "        save_strategy=\"epoch\", # save model at the end of every epoch\n",
    "        save_total_limit=2,\n",
    "\n",
    "        # Disable some options for pinned memory and experiment tracking\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\", # Disable Weights & Biases logging\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 655
    },
    "id": "4qoLnJQdqMl4",
    "outputId": "0405009a-bb91-443c-9a3d-b0579e7ed7d2"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fBIlTZsKu2xt",
    "outputId": "17f4283f-e42f-4ce0-b4a7-732606b3afcc"
   },
   "outputs": [],
   "source": [
    "prompt = \"### Input: hz ro\\n### Output:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=20,\n",
    "    temperature=0.0,\n",
    "    do_sample=False,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qUAj9-wtH-9z",
    "outputId": "44ce0cb6-1c32-4516-deac-8e524122b9bb"
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(validation_file[i]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK1nzDEa_EIs"
   },
   "outputs": [],
   "source": [
    "# Test the fine-tuned model with 1000 names in a json file\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "all_answers = []\n",
    "for i in range(len(validation_file)):\n",
    "  original_name = validation_file[i]['input']\n",
    "\n",
    "  prompt = f\"### Input: {original_name}\\n### Output:\"\n",
    "\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "  outputs = model.generate(\n",
    "      **inputs,\n",
    "      max_new_tokens=20,   # small limit, we expect just one word\n",
    "      use_cache=True,\n",
    "      temperature=0.0,\n",
    "      do_sample=False,\n",
    "      top_p=0.9,\n",
    "    )\n",
    "\n",
    "  response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "  answer = response.split(\"### Output:\")[-1].strip()\n",
    "  all_answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1h5Tmq2OPyqm"
   },
   "outputs": [],
   "source": [
    "list_inputs = []\n",
    "list_labels = []\n",
    "for input_label_dict in validation_file:\n",
    "  list_inputs.append(input_label_dict['input'])\n",
    "  list_labels.append(input_label_dict['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzpuCOTieQnl",
    "outputId": "2a4c4cf4-e563-4ec6-a7c7-e7bcd51c6644"
   },
   "outputs": [],
   "source": [
    "print(len(list_inputs))\n",
    "print(len(list_labels))\n",
    "print(len(all_answers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcD_d90tNyFG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_df = pd.DataFrame({\n",
    "    'Original Name': list_inputs,\n",
    "    'Label': list_labels,\n",
    "    'Generated Name': all_answers\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gwVv428btziz",
    "outputId": "cc4808b9-94f6-447a-ebd7-6c4a679dc3f2"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(result_df)):\n",
    "  correct_name = result_df.iloc[i]['Label']\n",
    "  pred_name = result_df.iloc[i]['Generated Name'][1:-1]\n",
    "  if correct_name == pred_name:\n",
    "    count += 1\n",
    "accuracy = count / len(result_df) * 100\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NZa-Cm0tq-v"
   },
   "outputs": [],
   "source": [
    "result_df.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "AU9TMYLDtS4b",
    "outputId": "a3b115bb-a9ac-4543-e634-e88d91d0ab01"
   },
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "dKAI89qvqfIM",
    "outputId": "cab50622-b94b-4ce1-9a31-faa2fb1773bb"
   },
   "outputs": [],
   "source": [
    "!pip install -U \"protobuf==3.20.3\"\n",
    "!pip install -U sentencepiece packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7iBqwoTjqcsk",
    "outputId": "258c6506-bb03-46ac-cb8f-9fec028480b9"
   },
   "outputs": [],
   "source": [
    "!rm -rf llama.cpp && git clone https://github.com/ggerganov/llama.cpp.git && cd llama.cpp && cmake -B build -DCMAKE_BUILD_TYPE=Release && cmake --build build -j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c035559de61b4326a29ff6fcb1fe5637",
      "39605b3316424dbe97927e3085c1fdd5",
      "5faf70f1ac77476a8d0cd03455d3cb21",
      "e87b93d850074607b10d11678b99af1d",
      "83291c174ea847248423fb70d31ffb38",
      "1206a349a17c43afbfd2137e2f2add5c",
      "50fe08b9972d4c98b569747ba4c354b9",
      "c3432d06f5d74cf6a23ec7feb677e59b",
      "653def8d4ad5443c9f64a8a7076bdc8f",
      "f7445fbaab14409483a2ae38bcc01aea",
      "2a0f1d5fb3944f77a2b6ca136ab85048",
      "f4a68a4eb04647fbbbf5e59bdad42b0d",
      "8ab734fea6c741eb860fe74f85880c80",
      "5b8ccfb1b77c41ea98e57b99aa0833db",
      "76862f661ca64b40ab40770833e75375",
      "1604f8eac5904ad9964e3d23a5bfb07a",
      "f773273da40c40da8461b0101039def2",
      "92848efdedfb4590be24ee6acbb41a8e",
      "321a9cf783e2496e84d1e3cac69057aa",
      "0895a96b5532484b8595b0fad4653024",
      "b7b08f24d9d34d61ab9fbe48ae60348f",
      "b86db25d67744ee7966326e22efbe34f",
      "5234f357f416468ab7a3ea853ff2c0a4",
      "7a198b000d8a459dad50a8a178f110a6",
      "2243a3516e4b488f94f073e2c14930e8",
      "5ccb89aacc474535911585eef240cf89",
      "f2c5079895e0451b8adba99b90b410cc",
      "088532f197c6440c92d4de4c66226af1",
      "df5fde336cb741dc8968794e079e4342",
      "1a22bb80c9dc4b4da3e5476132fcfbad",
      "423da7782770465eada72f24a6375c0a",
      "a60154da53a74261b552b792212363fa",
      "c55e5a88db26435d9855a34b14a4e594",
      "a1ead71b62834a2c8760c68705991931",
      "e97bb6069e3c4866bfb853c1697d7825",
      "71b512b88bd44ebebd6a22ab41b30ed7",
      "51312a43aa294c45a6931908240947c4",
      "8cc8834ee60a4d739d285811e3a3edac",
      "c3bcc38c313e4f90ae90d9b7cc86be68",
      "bd01c0f230954ff297f9c03f07f550e0",
      "fdeb06bd9f6f4db78264b079c4e8ada3",
      "01b7b2093cb84c62a7a8451bd7a74644",
      "35e79829a05644a78a85c5b7028e0748",
      "09bd5b57b61a41f9ae3690ca15c673ba",
      "80ba3cb4a9344dc291e354a4cc015d73",
      "464b7627ee154b4fa6b2d1f5b19d05c8",
      "09088b5a963d476797a100079fef2827",
      "85065fbec2fe43e98cc8f7df82eeeab6",
      "e6a4a6ba2ccd4acf865a2922439e5054",
      "0612ccaf53f54b978ae5b9f9e0e2cce7",
      "11702063d9b04e95a804cb6e728b01a5",
      "b82e603249cb4a59846f66e63d1c3ac4",
      "70456b11cdbf48a6b7166d9b6fabe9df",
      "b7954cc69b834458b504c7a4274b33b2",
      "bd3d5c33b4324cd5804ff1beb4d30b0a"
     ]
    },
    "id": "4GnooneaxS1n",
    "outputId": "9f5a701b-ab4d-45df-d564-a9c4ed84e1a9"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained_gguf(\n",
    "    \"gguf_model\",\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2__UBiz3wS4"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "gguf_files = [f for f in os.listdir(\"gguf_model\") if f.endswith(\".gguf\")]\n",
    "if gguf_files:\n",
    "    gguf_file = os.path.join(\"gguf_model\", gguf_files[0])\n",
    "    print(f\"Downloading: {gguf_file}\")\n",
    "    files.download(gguf_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
